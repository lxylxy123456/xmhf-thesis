\subsection{Virtualizing NMIs}
\label{sec:impl_nmi}

Non-maskable interrupts (NMIs) are required by both \XMHF64 and the rich OS/Apps, so \XMHF64 must virtualize NMIs. \XMHF64 uses NMIs to implement quiescing. The rich OS/Apps require NMIs to receive notification on non-recoverable hardware errors. Linux also implements watchdog using NMI \cite{linux_nmi_watchdog}.

NMIs can arrive at the CPU at any time when \XMHF64 and rich OS/Apps are running. If an NMI arrives when \XMHF64 is running, the CPU invokes the NMI interrupt handler in \XMHF64.\footnote{The NMI interrupt handler breaks the atomicity of the \XMHF64 code that the NMI interrupts. Thus, both XMHF and \XMHF64 require manual auditing of the NMI interrupt handler during formal verification.} If an NMI arrives when the rich OS/Apps are running, the CPU invokes the NMI VM-exit handler\footnote{In this section, we use ``VM-exit'' instead of ``intercept'' to prevent confusion between ``NMI interrupt handler'' and ``NMI intercept handler''. The NMI interrupt handler in \XMHF64 is not one of the NMI VM-exit handlers (i.e., NMI intercept handlers) in \XMHF64. The former is an interrupt handler in x86, while the latter is a VM-exit handler in VMX.} in \XMHF64.

Both the NMI interrupt handler and NMI VM-exit handler in \XMHF64 implement the same NMI virtualization logic. This NMI virtualization logic checks whether another CPU is requesting quiescing. If so, it invokes the logic for quiescing, which involves entering a busy wait loop until the other CPU ends the quiescing. If not, it modifies VMCS to inject the NMI into the rich OS/Apps.

To prevent the NMI interrupt handler or NMI VM-exit handler from being interrupted by another NMI, Intel defines the behavior of NMI blocking. NMIs become blocked at the start of the NMI interrupt handler and NMI VM-exit handler. NMIs become unblocked when the CPU executes the interrupt return (IRET) instruction. The CPU only invokes the NMI interrupt handler or NMI VM-exit handler when NMIs are blocked \cite{intel_sdm}. Thus, the NMI interrupt handlers in \XMHF64 and rich OS/Apps do not need to be reentrant.

\subsubsection{Injecting NMIs into the Rich OS/Apps}

One challenge of implementing NMI virtualization is that \XMHF64 must respect NMI blocking of the rich OS/Apps. After \XMHF64 injects an NMI into the rich OS/Apps, it must wait until the rich OS/Apps unblock NMIs before injecting another NMI into the rich OS/Apps.

To correctly inject NMIs into the rich OS/Apps, \XMHF64 enables virtual NMIs in the VMX configuration. With virtual NMIs enabled, VMX monitors whether the rich OS/Apps are blocking NMIs. To inject an NMI into the rich OS/Apps, \XMHF64's NMI virtualization logic sets the NMI-window exiting bit in VMCS. This bit triggers an NMI window VM-exit when the rich OS/Apps unblock NMIs. In the NMI window VM-exit handler, \XMHF64 injects the NMI into the rich OS/Apps.

One compatibility bug in XMHF arises from injecting NMIs into the rich OS/Apps without checking whether NMIs are blocked by the rich OS/Apps. XMHF does not enable virtual NMIs, and its NMI virtualization logic injects NMIs into the rich OS/Apps directly, rather than using the NMI window VM-exit. Since Linux's NMI interrupt handler is not reentrant, injecting NMIs into Linux while NMIs are blocked can result in undefined behavior. For example, when we run Debian 11 as the rich OS in XMHF and frequently send NMIs to the CPU, we observe that Debian hangs in the NMI interrupt handler after XMHF injects the NMIs into it.

\subsubsection{Simulating NMI Blocking}

Another challenge is a race condition between the NMI interrupt handler and the non-NMI VM-exit handler that is interrupted by the NMI. Suppose the non-NMI VM-exit handler is modifying VMCS when an NMI arrives and the CPU invokes the NMI interrupt handler. The NMI interrupt handler also modifies VMCS to inject the NMI into the rich OS/Apps. The race condition happens because both the non-NMI VM-exit handler and the NMI interrupt handler modify VMCS.

To prevent this race condition, the non-NMI VM-exit handler must identify critical sections in its logic. During the execution of any critical section, the NMI interrupt handler must not execute the NMI virtualization logic, which involves either executing the quiescing logic or modifying VMCS to inject the NMI into the rich OS/Apps, depending on whether quiescing is requested. \XMHF64 simulates NMI blocking during critical section execution. Each CPU defines two per-CPU variables: $B$, a flag indicating whether the CPU is currently simulating NMI blocking, and $V$, a counter recording the number of NMIs that have visited the CPU but have not been handled by the NMI virtualization logic. \XMHF64 initializes $B$ to false and $V$ to $0$.

\begin{algorithm}
\caption{NMI Interrupt Handler}
\label{alg:nmi_handler}
\begin{algorithmic}
	\IF{$B =$ true}
		\STATE $V \Leftarrow V + 1$
	\ELSE[$B =$ false]
		\STATE Execute NMI virtualization logic
	\ENDIF
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:nmi_handler} presents the implementation of the NMI interrupt handler. If the CPU is simulating NMI blocking (i.e., the critical section is running), the NMI interrupt handler increments $V$ to indicate that an NMI has visited but has not been handled yet. The execution of the NMI virtualization logic is delayed until the critical section completes. Otherwise, the NMI interrupt handler executes the NMI virtualization logic. The NMI interrupt handler is not reentrant, and the hardware ensures that NMIs are blocked when the NMI interrupt handler is executing.

\begin{algorithm}
\caption{Protecting the NMI Critical Section}
\label{alg:nmi_critical_section}
\begin{algorithmic}
	\REQUIRE $B =$ false
	\ENSURE $B =$ false
	\STATE $B \Leftarrow$ true
	\STATE Execute critical section
	\STATE $B \Leftarrow$ false
	\WHILE{$V > 0$}
		\STATE $V \Leftarrow V - 1$
		\STATE $B \Leftarrow$ true
		\STATE Execute NMI virtualization logic
		\STATE $B \Leftarrow$ false
	\ENDWHILE
\end{algorithmic}
\end{algorithm}

Algorithm~\ref{alg:nmi_critical_section} shows how the critical section is protected. Before executing the critical section, the non-NMI VM-exit handler sets $B$ to true to simulate NMI blocking. Thus, if an NMI interrupt arrives, the NMI interrupt handler does not execute the NMI virtualization logic, ensuring that the critical section and the NMI virtualization logic are mutually exclusive. After the critical section completes, the algorithm checks whether $V$ is greater than $0$, indicating that one or more NMIs have arrived but have not been handled yet. If so, the algorithm calls the NMI virtualization logic to ensure that all NMIs are eventually handled.

In Algorithms~\ref{alg:nmi_handler} and \ref{alg:nmi_critical_section}, CPU-local variables $B$ and $V$ must be accessed atomically. Thus, we utilize x86's atomic instructions when increasing and decreasing $V$. We also use volatile variables and memory fences to prevent the compiler and the CPU from reordering or changing these algorithms.

This race condition results in a compatibility bug in XMHF. After the NMI interrupt handler modifies VMCS to inject the NMI interrupt into the rich OS/Apps, the non-NMI VM-exit handler that has been interrupted overwrites the modified VMCS fields. As a result, the NMI interrupt is never injected into the rich OS/Apps. Note that this bug does not violate memory integrity of the micro-hypervisor, as the NMI interrupt handler does not modify VMCS fields related to EPT.

