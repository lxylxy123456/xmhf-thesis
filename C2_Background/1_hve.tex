\section{Hardware Virtualization}
\label{sec:bg_hvm}

Modern x86 CPUs are equipped with the hardware virtualization extension, which is commonly utilized by hypervisors to efficiently run virtual machines (VMs). Popular hypervisors include VMware, VirtualBox, KVM, and Hyper-V. In Intel CPUs, the hardware virtualization extension is implemented via virtual-machine extensions (VMX) \cite{intel_sdm}. In this thesis, we use the term ``hardware virtualization extension'' to refer to general x86 hardware virtualization technologies. We use the term ``VMX'' to refer specifically to hardware virtualization in Intel CPUs.

The hardware virtualization extension defines two modes for the CPU: host mode and guest mode. The hypervisor runs in host mode, and the virtual machines run in guest mode. When an event occurs in guest mode, the hardware generates an intercept and switches to host mode. In VMX, VM-entry refers to the switch from host mode to guest mode, which occurs when the hypervisor executes the launch virtual machine (VMLAUNCH) and resume virtual machine (VMRESUME) instructions. VM-exit refers to the switch from guest mode to host mode, which occurs when an intercept is generated.

The hardware virtualization extension defines the virtual-machine control structure (VMCS), a data structure which transitions the host mode environment to the guest mode environment. In VMX, VMCS contains four types of fields. The guest-state fields specify the state of the guest after VM-entry. The host-state fields specify the state of the host after VM-exit. The control fields configure the CPU's behavior during guest mode. The information fields contain information about the last VM-exit.

The hardware virtualization extension defines the hardware page table (HPT) to translate memory addresses from guest mode to host mode. In VMX, HPT is implemented through the extended page table (EPT). The structure of EPT is similar to the page tables used by modern operating systems to support virtual memory. Similar to page tables, the hardware caches EPT entries in the translation lookaside buffer (TLB). Thus, the hypervisor must execute the invalidate translations derived from EPT (INVEPT) instruction to invalidate the TLB after changing or removing EPT entries.

\subsection{Nested Virtualization}

Nested virtualization refers to running a hypervisor inside another hypervisor. The hardware virtualization extension of x86 CPUs does not support nested virtualization in hardware. Thus, hypervisors supporting nested virtualization must virtualize the hardware virtualization extension. These hypervisors must also virtualize other hardware resources, such as I/O devices.

KVM \cite{ben2010turtles} is a hypervisor that implements nested virtualization. KVM defines multiple levels of virtualization. KVM runs in L0, the guest hypervisors run in L1, and guests of the L1 guest hypervisors run in L2. L0 runs in host mode, and all other levels run in guest mode.

To run an L2 guest, KVM must virtualize its VMCS. The L1 hypervisor provides VMCS12, which defines the transition between L1 and L2. KVM provides VMCS01, which defines the transition between L0 and L1. To implement nested virtualization, KVM must compute VMCS02, which defines the transition between L0 and L2.

KVM must virtualize intercepts from the L2 guest, which occur when events happen during L2 guest execution. If the event is related to KVM, KVM handles the intercept and resumes L2. Otherwise, KVM forwards the event to the L1 hypervisor by injecting the intercept into the L1 hypervisor.

KVM must also virtualize HPT. The L1 hypervisor provides HPT12, which translates L2 memory addresses to L1 memory addresses. KVM provides HPT01, which translates L1 memory addresses to L0 memory addresses. To run the L2 guest, KVM must build HPT02, which translates L2 memory addresses to L0 memory addresses. Initially, HPT02 is empty, and KVM builds it on-the-fly. When the L2 guest accesses a memory address that corresponds to an empty entry in HPT02 and triggers an HPT violation intercept, KVM computes the entry and adds it to HPT02. Whenever the L1 hypervisor modifies HPT12, KVM invalidates the corresponding entries in HPT02.

